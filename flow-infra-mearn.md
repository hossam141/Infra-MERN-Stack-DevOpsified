# üöÄ Step 1: Provision EC2 Instance with DevOps Toolchain

## üñ•Ô∏è EC2 Instance Setup

- **Operating System**: Ubuntu 22.04
- **Instance Type**: t3.medium or higher recommended
- **Storage**: 30 GB GP2 (minimum)
- **Security Group**: Open ports 22 (SSH), 8080 (Jenkins), 9000 (SonarQube), 80/443 (as needed)
- **IAM Role (Instance Profile)**: Attach an IAM role with the following policies:
  - `AmazonEC2FullAccess`
  - `AmazonEKSClusterPolicy`
  - `AmazonEKSWorkerNodePolicy`
  - `AmazonECRFullAccess`
  - `AmazonS3FullAccess`
  - `CloudWatchAgentServerPolicy`
  - `IAMFullAccess` *(optional / can use least privilege)*

### üëá User Data Script

Paste the following script into the **User Data** field during EC2 provisioning (Advanced options):

```bash
#!/bin/bash
# Ubuntu 22.04 EC2 Bootstrapping Script

# Java Installation
sudo apt update -y
sudo apt install openjdk-17-jre -y
sudo apt install openjdk-17-jdk -y
java --version

# Jenkins Installation
curl -fsSL https://pkg.jenkins.io/debian/jenkins.io-2023.key | sudo tee /usr/share/keyrings/jenkins-keyring.asc > /dev/null
echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] https://pkg.jenkins.io/debian binary/ | sudo tee /etc/apt/sources.list.d/jenkins.list > /dev/null
sudo apt-get update -y
sudo apt-get install jenkins -y

# Docker Installation
sudo apt install docker.io -y
sudo usermod -aG docker jenkins
sudo usermod -aG docker ubuntu
sudo systemctl restart docker
sudo chmod 777 /var/run/docker.sock

# Optional: Jenkins as Docker Container
# docker run -d -p 8080:8080 -p 50000:50000 --name jenkins-container jenkins/jenkins:lts

# SonarQube as Docker Container
docker run -d --name sonar -p 9000:9000 sonarqube:lts-community

# AWS CLI Installation
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
sudo apt install unzip -y
unzip awscliv2.zip
sudo ./aws/install

# kubectl Installation
sudo apt install curl -y
sudo curl -LO "https://dl.k8s.io/release/v1.28.4/bin/linux/amd64/kubectl"
sudo chmod +x kubectl
sudo mv kubectl /usr/local/bin/
kubectl version --client

# eksctl Installation
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin
eksctl version

# Terraform Installation
wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
sudo apt update
sudo apt install terraform -y

# Trivy Installation
sudo apt-get install wget apt-transport-https gnupg lsb-release -y
wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list
sudo apt update
sudo apt install trivy -y

# Helm Installation
sudo snap install helm --classic

---

# ‚öôÔ∏è Step 2: Jenkins Plugins Setup for AWS & Terraform Integration

## ‚úÖ Plugins Installed

### 1. **AWS Credentials Plugin**
- **Name**: `AWS Credentials`
- **Purpose**: 
  - Stores IAM Access Key ID & Secret Access Key in Jenkins Credentials store.
  - Supports MFA tokens and IAM Roles (advanced use).

### 2. **Pipeline: AWS Steps Plugin**
- **Name**: `Pipeline: AWS Steps`
- **Purpose**: 
  - Adds pipeline support to interact with AWS API using scripted or declarative pipelines.

### 3. **Terraform Plugin**
- **Name**: `Terraform`
- **Purpose**: 
  - Allows you to run `terraform init`, `plan`, `apply`, etc., as Jenkins build steps.
  - Useful for IAC pipelines.

---

## üîê Jenkins Credentials Setup for AWS

- **Navigate to**: `Manage Jenkins > Credentials > Global > Add Credentials`
- **Kind**: `AWS Credentials`
- **ID**: `aws-creds` *(use this ID in pipeline scripts)*
- **Access Key ID**: `<Your AWS Access Key>`
- **Secret Access Key**: `<Your AWS Secret Key>`
- **Scope**: `Global`

---

## üß∞ Terraform Configuration in Jenkins

- **Navigate to**: `Manage Jenkins > Global Tool Configuration`
- Under **Terraform installations**:
  - Click `Add Terraform`
  - **Name**: `terraform`
  - **Install Directory**: `/usr/bin/terraform` *(already installed in EC2)*
  - **Install Automatically**: (Leave unchecked)



# üõ†Ô∏è Step 3: Provisioning EKS Cluster using Jenkins + Terraform

In this step, we built an automated Jenkins pipeline to provision an **Amazon EKS Cluster** using **Terraform** code stored in GitHub. Below is the detailed and descriptive documentation of what was done, suitable for publishing in a professional portfolio or GitHub repository.

---

## üìÅ Folder Structure

Our infrastructure code is organized as follows:

```
eks/
‚îú‚îÄ‚îÄ backend.tf             # Remote backend configuration (e.g., S3 + DynamoDB)
‚îú‚îÄ‚îÄ dev.tfvars             # Environment-specific variables for 'dev'
‚îú‚îÄ‚îÄ main.tf                # Main Terraform configuration file calling modules
‚îú‚îÄ‚îÄ variables.tf           # Variable declarations
‚îú‚îÄ‚îÄ .terraform.lock.hcl    # Provider lock file

module/
‚îú‚îÄ‚îÄ eks.tf                 # EKS cluster definition
‚îú‚îÄ‚îÄ vpc.tf                 # VPC setup
‚îú‚îÄ‚îÄ iam.tf                 # IAM roles and policies
‚îú‚îÄ‚îÄ gather.tf              # Data sources or outputs
‚îú‚îÄ‚îÄ variables.tf           # Module-specific variables
```

---

## üß© Jenkins Pipeline Setup (Declarative Pipeline)

We created a new Jenkins pipeline job named `Infrastructure-Job`.

**Steps to Create the Pipeline Job:**
- Navigate to **Jenkins Dashboard > New Item**.
- Enter name: `Infrastructure-Job`.
- Choose project type: **Pipeline**.
- Click OK.

---

## üßæ Jenkinsfile Logic

A declarative pipeline was written directly in the Jenkins UI under **Pipeline > Script** section. Here's what each stage does:

### üîß Parameters Setup

- `Environment`: String parameter with default value `dev`.
- `Terraform_Action`: Choice parameter with options `plan`, `apply`, `destroy`.

### üöÄ Pipeline Stages Breakdown

1. **Preparing**: Basic echo to mark the start of the pipeline.
   ```sh
   echo Preparing
   ```

2. **Git Pulling**: Clone the GitHub repository that holds the Terraform code.
   ```groovy
   git branch: 'master', url: 'https://github.com/AmanPathak-DevOps/EKS-Terraform-GitHub-Actions.git'
   ```

3. **Init**: Initialize Terraform in the `eks/` directory using AWS credentials from Jenkins.
   ```sh
   terraform -chdir=eks/ init
   ```

4. **Validate**: Validate the Terraform configuration.
   ```sh
   terraform -chdir=eks/ validate
   ```

5. **Action (plan/apply/destroy)**:
   Dynamically runs one of the following based on the chosen `Terraform_Action` parameter:
   - `plan`: Shows the resources to be created
   - `apply`: Provisions the infrastructure on AWS
   - `destroy`: Tears down the infrastructure

   ```sh
   terraform -chdir=eks/ plan -var-file=dev.tfvars
   terraform -chdir=eks/ apply -var-file=dev.tfvars -auto-approve
   terraform -chdir=eks/ destroy -var-file=dev.tfvars -auto-approve
   ```

---

## ‚úÖ Pipeline Execution Summary

- The job was executed successfully with `Terraform_Action = plan`.
- Jenkins cloned the repo, initialized the configuration, validated it, and ran the plan.
- The Terraform plan showed `38 to add, 0 to change, 0 to destroy` ‚Äî indicating new infrastructure ready to be provisioned.
- On choosing `apply`, the resources (including VPC, EKS, IAM roles) were deployed to AWS.

---

## üîê Credentials & Plugins

- **AWS Credentials**: Configured in Jenkins using ID `aws-creds`.
- **Installed Plugins**:
  - Pipeline
  - Pipeline: Stage View
  - Docker Pipeline (optional for future steps)

---

## üìå Conclusion

This step sets up a fully working CI pipeline to manage infrastructure as code (IaC) for EKS. By integrating Jenkins with Terraform, we can control deployments through parameters and automate cloud provisioning with high visibility.

This is an essential building block in our end-to-end DevOps pipeline for a MERN application.


# üõ°Ô∏è Step 4: Create & Configure Jump Server in EKS VPC

After provisioning our EKS cluster, we need a secure point of entry into the private network hosting the cluster. This is achieved by launching and configuring a **Jump Server** inside the same VPC as our EKS cluster.

### üõ†Ô∏è Launching the EC2 Jump Server

We launched a new EC2 instance using the following settings:
- **Name**: Jump-Server
- **AMI**: Ubuntu 22.04 LTS
- **Instance Type**: `t2.medium`
- **VPC**: `dev-medium-vpc`
- **Subnet**: Public subnet (`dev-medium-subnet-public-1`)
- **Public IP**: Enabled (Auto-assigned)
- **Security Group**: Created a new one to allow SSH access

### ‚öôÔ∏è EC2 User Data Configuration

We provided a User Data script during instance launch to automatically install the following essential tools on boot:
- AWS CLI
- kubectl
- Helm
- eksctl

This ensures the jump server is pre-equipped to interact with EKS clusters and manage deployments via Kubernetes tools.

### ‚úÖ Finalizing Jump Server Configuration

Once the instance was up and running, we connected to it via SSH and performed the following:

#### 1. **Configure AWS CLI credentials**:
```bash
aws configure
# Provided Access Key, Secret Key, Region (us-east-1), and output format (json)
```

#### 2. **Update kubeconfig to access the EKS cluster**:
```bash
aws eks update-kubeconfig --name dev-medium-eks-cluster --region us-east-1
```

This command added the EKS cluster context to the kubeconfig file located at `/home/ubuntu/.kube/config`.

#### 3. **Verify cluster connectivity**:
```bash
kubectl get nodes
```

We successfully listed the worker nodes, which confirms the jump server can interact with the EKS cluster.



# ‚öôÔ∏è Step 5: Deploying AWS Load Balancer Controller in EKS

The AWS Load Balancer Controller allows Kubernetes applications to use AWS Elastic Load Balancers (ALBs/NLBs) to expose services externally. Here‚Äôs how we deployed and verified it in our EKS cluster.

---

## üßæ IAM Policy Setup

We first downloaded and created an IAM policy required by the AWS Load Balancer Controller:

```bash
curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.4/docs/install/iam_policy.json
```

Then we created the IAM policy in AWS Management Console or via AWS CLI to be associated with the service account.

---

## üîê Create IAM Service Account Using `eksctl`

To grant Kubernetes permission to use the Load Balancer features, we created a Kubernetes service account bound to the IAM policy:

```bash
eksctl create iamserviceaccount   --cluster dev-medium-eks-cluster   --namespace kube-system   --name aws-load-balancer-controller   --attach-policy-arn arn:aws:iam::<ACCOUNT_ID>:policy/AWSLoadBalancerControllerIAMPolicy   --approve
```

We verified the service account creation:

```bash
kubectl get sa -n kube-system
```

---

## üì¶ Add Helm Repository for AWS Controller

We added the EKS Helm chart repository and updated it:

```bash
helm repo add eks https://aws.github.io/eks-charts
helm repo update
```

We confirmed it with:

```bash
helm repo list
```

---

## üöÄ Deploy AWS Load Balancer Controller via Helm

Finally, we installed the controller into our EKS cluster using Helm:

```bash
helm install aws-load-balancer-controller eks/aws-load-balancer-controller   -n kube-system   --set clusterName=dev-medium-eks-cluster   --set serviceAccount.create=false   --set serviceAccount.name=aws-load-balancer-controller
```

---

## ‚úÖ Verify the Deployment

We confirmed successful deployment using:

```bash
kubectl get deployment -n kube-system aws-load-balancer-controller
```

You should see the controller running with READY status (e.g., 2/2 pods available).

---

This completes the setup of the AWS Load Balancer Controller in our EKS environment. This controller is now capable of managing ALBs for Kubernetes ingress resources based on your application deployments.


# üöÄ Step 6: Argo CD Deployment on Amazon EKS

This guide documents the step-by-step instructions to deploy Argo CD on an Amazon EKS cluster and expose its UI using a LoadBalancer.

---

## 1. Create Namespace

```bash
kubectl create namespace argocd
```

---

## 2. Install Argo CD in the Namespace

```bash
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.4.7/manifests/install.yaml
```

This will install:

* CRDs (Custom Resource Definitions)
* Service Accounts
* Roles, RoleBindings, ClusterRoles, ClusterRoleBindings
* ConfigMaps
* Secrets
* Services
* Deployments and StatefulSets

---

## 3. Verify All Components Are Running

```bash
kubectl get all -n argocd
```

Ensure all pods show `STATUS = Running`.

---

## 4. Get Argo CD Server Service Info

```bash
kubectl get svc -n argocd
```

Look for the `argocd-server` service. Initially, it is of `ClusterIP` type.

---

## 5. Edit the Argo CD Server Service to Expose It via LoadBalancer

```bash
kubectl edit svc argocd-server -n argocd
```

Modify this line:

```yaml
type: ClusterIP
```

to:

```yaml
type: LoadBalancer
```

> **Note:** When changing the `argocd-server` service type to `LoadBalancer`, the AWS Cloud Controller Manager component of the EKS cluster is responsible for provisioning the AWS ALB. This is **not** handled by the AWS Load Balancer Controller that we might have installed separately.

---

## 6. Wait for LoadBalancer to be Provisioned

After saving the change, check again:

```bash
kubectl get svc -n argocd
```

Copy the `EXTERNAL-IP` of `argocd-server`. It may take a few minutes for AWS to provision it.

---

## 7. Get the Initial Admin Password

```bash
kubectl get secrets -n argocd
kubectl get secret argocd-initial-admin-secret -n argocd -o yaml
```

Then decode it:

```bash
echo <base64-password> | base64 --decode
```

---

## 8. Access Argo CD UI

Go to `http://<EXTERNAL-IP>` or `https://<EXTERNAL-IP>` in your browser.
Login with:

* **Username**: `admin`
* **Password**: `<decoded-password>`

You should see the Argo CD dashboard.

---

‚úÖ You have now successfully deployed and accessed Argo CD on Amazon EKS!
